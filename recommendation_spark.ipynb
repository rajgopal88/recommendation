{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nImporting the multiarray numpy extension module failed.  Most\nlikely you are trying to import a failed build of numpy.\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\nfiles not under version control).  Otherwise reinstall numpy.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so: undefined symbol: _Py_ZeroStruct",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dfb8dfb88ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     __all__ = ['add_newdocs',\n\u001b[1;32m    144\u001b[0m                \u001b[0;34m'ModuleDeprecationWarning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/add_newdocs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_tricks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunction_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/type_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m            'common_type']\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mobj2sctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m files not under version control).  Otherwise reinstall numpy.\n\u001b[1;32m     23\u001b[0m \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nImporting the multiarray numpy extension module failed.  Most\nlikely you are trying to import a failed build of numpy.\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\nfiles not under version control).  Otherwise reinstall numpy.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkConf, SparkContext \n",
    "\n",
    "def parsePurchases(line):\n",
    "    \"\"\"\n",
    "    Parses a purchases record in Purchases format user_Id::sku_Id::purchases::timestamp .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))\n",
    "\n",
    "def parseSku(line):\n",
    "    \"\"\"\n",
    "    Parses a sku record in Sku format skuId::sku_Title .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return int(fields[0]), fields[1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) != 3):\n",
    "        print(\"Usage: /path/to/spark/bin/spark-submit --driver-memory 2g \" + \\\n",
    "          \"recommendaion.py DataDir personalPurchasesFile\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # set up environment\n",
    "    conf = SparkConf() \\\n",
    "      .setAppName(\"Reccomendation\") \\\n",
    "      .set(\"spark.executor.memory\", \"2g\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    # load personal purchases\n",
    "    myPurchases = loadPurchases(sys.argv[2])\n",
    "    myPurchasesRDD = sc.parallelize(myPurchases, 1)\n",
    "    \n",
    "    # load purchases and product titles\n",
    "\n",
    "    PurchasesHomeDir = sys.argv[1]\n",
    "\n",
    "    # ratings is an RDD of (last digit of timestamp, (user_Id, sku_Id, purchases))\n",
    "    purchases = sc.textFile(join(PurchasesHomeDir, \"purchases.dat\")).map(parsePurchases)\n",
    "\n",
    "    # sku is an RDD of (sku_Id, sku_Title)\n",
    "    sku = dict(sc.textFile(join(PurchasesHomeDir, \"sku.dat\")).map(parseSku).collect())\n",
    "\n",
    "    numPurchases = purchases.count()\n",
    "    numUsers = purchases.values().map(lambda r: r[0]).distinct().count()\n",
    "    numSkus = purchases.values().map(lambda r: r[1]).distinct().count()\n",
    "\n",
    "    print(\"Got %d ratings from %d users on %d movies.\", numPurchases, numUsers, numSkus)\n",
    "\n",
    "    # split ratings into train (60%), validation (20%), and test (20%) based on the \n",
    "    # last digit of the timestamp, add myRatings to train, and cache them\n",
    "\n",
    "    # training, validation, test are all RDDs of (userId, movieId, rating)\n",
    "\n",
    "    numPartitions = 4\n",
    "    training = purchases.filter(lambda x: x[0] < 6) \\\n",
    "      .values() \\\n",
    "      .union(myPurchasesRDD) \\\n",
    "      .repartition(numPartitions) \\\n",
    "      .cache()\n",
    "\n",
    "    validation = purchases.filter(lambda x: x[0] >= 6 and x[0] < 8) \\\n",
    "      .values() \\\n",
    "      .repartition(numPartitions) \\\n",
    "      .cache()\n",
    "\n",
    "    test = purchases.filter(lambda x: x[0] >= 8).values().cache()\n",
    "\n",
    "    numTraining = training.count()\n",
    "    numValidation = validation.count()\n",
    "    numTest = test.count()\n",
    "\n",
    "    print(\"Training: %d, validation: %d, test: %d\", numTraining, numValidation, numTest)\n",
    "\n",
    "    # train models and evaluate them on the validation set\n",
    "\n",
    "    ranks = [8, 12]\n",
    "    lambdas = [0.1, 10.0]\n",
    "    numIters = [10, 20]\n",
    "    bestModel = None\n",
    "    bestValidationRmse = float(\"inf\")\n",
    "    bestRank = 0\n",
    "    bestLambda = -1.0\n",
    "    bestNumIter = -1\n",
    "\n",
    "    for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "        model = ALS.train(training, rank, numIter, lmbda)\n",
    "        validationRmse = computeRmse(model, validation, numValidation)\n",
    "        print(\"RMSE (validation) = %f for the model trained with \", validationRmse + \\\n",
    "            \"rank = %d, lambda = %.1f, and numIter = %d.\", rank, lmbda, numIter)\n",
    "        if (validationRmse < bestValidationRmse):\n",
    "            bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "\n",
    "    testRmse = computeRmse(bestModel, test, numTest)\n",
    "\n",
    "    # evaluate the best model on the test set\n",
    "    print(\"The best model was trained with rank = %d and lambda = %.1f, \", bestRank, bestLambda \\\n",
    "      + \"and numIter = %d, and its RMSE on the test set is %f.\", bestNumIter, testRmse)\n",
    "\n",
    "    # compare the best model with a naive baseline that always returns the mean rating\n",
    "    meanPurchase = training.union(validation).map(lambda x: x[2]).mean()\n",
    "    baselineRmse = sqrt(test.map(lambda x: (meanPurchase - x[2]) ** 2).reduce(add) / numTest)\n",
    "    improvement = (baselineRmse - testRmse) / baselineRmse * 100\n",
    "    print(\"The best model improves the baseline by %.2f\", improvement + \"%.\")\n",
    "\n",
    "    # make personalized recommendations\n",
    "\n",
    "    myRatedSkuIds = set([x[1] for x in myPurchases])\n",
    "    candidates = sc.parallelize([m for m in movies if m not in myRatedSkuIds])\n",
    "    predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect()\n",
    "    recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]\n",
    "\n",
    "    print(\"Movies recommended for you:\"\n",
    "    for i in xrange(len(recommendations)):\n",
    "        print (\"%2d: %s\" % (i + 1, sku[recommendations[i][1]])).encode('ascii', 'ignore'))\n",
    "\n",
    "    # clean up\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
